{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hangsheng0625/AI_story_generator/blob/main/Week_01_FIT5215_Tute1b_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsbuejjOgQUa"
      },
      "source": [
        "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2024)</span>\n",
        "***\n",
        "*Lecturer (Malaysia):*  **Dr Arghya Pal** | arghya.pal@monash.edu <br/>\n",
        "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>\n",
        "\n",
        "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
        "*Lecturer (Clayton):* **Prof Dinh Phung** | dinh.phung@monash.edu <br/>\n",
        "  <br/>\n",
        "<br/>\n",
        "School of IT and Faculty of Information Technology, Monash University, Malaysia and Australia\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfD8qvjPgZyh"
      },
      "source": [
        "# Tutorial 1b: Logistic Regression with PyTorch\n",
        "\n",
        "\n",
        "This tutorial aims to introduce the Logistic Regression which can be regarded as a feed-forward neural network with one layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibELGzvaikdK"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KvWnbflf5da"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler  # for feature scaling\n",
        "from sklearn.model_selection import train_test_split  # for train/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogwAZF7jD30"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edfcjVt7jmND"
      },
      "source": [
        "We first load the `breast cancer` dataset from `sklean` datasets and then split into 80% for training and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzXzzS1cgqnA",
        "outputId": "16a2b25d-5627-4722-d2b8-592655a81bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of samples: 569, number of features: 30\n"
          ]
        }
      ],
      "source": [
        "# Prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(f'number of samples: {n_samples}, number of features: {n_features}')\n",
        "\n",
        "# split data to 80% for training and 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw7fZ63623i_"
      },
      "source": [
        "**<span style=\"color:red\">Exercise 1</span>:** Write the code to print out the first 10 feature vectors in `X_train` and `y_train`. Write the code to show the unique labels in `y_train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-mi6wAT6Iav"
      },
      "outputs": [],
      "source": [
        "#Your answer here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb0to6spkf30"
      },
      "source": [
        "We use `StandardScaler()` from `sklearn` to normalize the training/testing sets. We convert the training/testing numpy arrays to PyTorch arrays and then reshape them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo8WXCEBjQ2H"
      },
      "outputs": [],
      "source": [
        "# scale data\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# convert to tensors\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# reshape y tensors\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3FScpK3nrsW"
      },
      "source": [
        "## Training/Testing Procedure\n",
        "\n",
        "We now present the `fundamental workflow of PyTorch` including training a model based on the training set and testing the trained model on the testing set. This fundamental workflow is the same for various PyTorch models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9TmWoxulW4j"
      },
      "source": [
        "### Prepare Model\n",
        "\n",
        "First, we need to declare and define a model, which is a computational graph showing how to compute the model output from the input vector $x$. Specifically, given a data point $x$ (i.e., [1,30]) a batch $x$ (i.e., [64,30]), or even the entire training set $x$ (i.e., [569,30]), we compute\n",
        "- logits = xW + b\n",
        "- pred_probs = softmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VeNzXPcjR_-"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "# f = wx + b, softmax at the end\n",
        "class LogisticRegression(nn.Module):\n",
        "\n",
        "    def __init__(self, n_input_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x)\n",
        "        pred_probs = torch.nn.Softmax(dim=-1)(logits) #for asking question only\n",
        "        return logits #return the logits\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LogisticRegression(n_features).to(device)  #load the model to the current device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvtTrPQ0DT7g"
      },
      "source": [
        "**<span style=\"color:red\">Exercise 2</span>:** Explain the forward function. What are the meanings and dimensions of `logits` and `pred_probs`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "179EXh3XmDqe"
      },
      "source": [
        "### Prepare Loss and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rSEFHvn4NAq"
      },
      "source": [
        "We declare `loss_fn` as the cross entropy loss. To train our logistic regression, we invoke the SGD optimizer with the learning rate $0.01$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NvIujMFlyP3"
      },
      "outputs": [],
      "source": [
        "# Loss and optimizer\n",
        "learning_rate = 0.01\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86QKl-TfmqQV"
      },
      "source": [
        "### Train Model By Feeding the Training Set All-in-Once"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPRYtcCl8vcl"
      },
      "source": [
        "We train the model in $100$ epochs (i.e., going through the entire training set $100$ times). Here in each epoch, we input entire training set to the model to compute the cross-entropy loss over the training set and then use the optimizer to update the model parameters (i.e., W and b)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I_P8kWLl9Da",
        "outputId": "a26b97f4-0be3-4d3a-ce7e-a3365e897398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss = 0.5223\n",
            "epoch: 20, loss = 0.3920\n",
            "epoch: 30, loss = 0.3230\n",
            "epoch: 40, loss = 0.2797\n",
            "epoch: 50, loss = 0.2497\n",
            "epoch: 60, loss = 0.2275\n",
            "epoch: 70, loss = 0.2103\n",
            "epoch: 80, loss = 0.1965\n",
            "epoch: 90, loss = 0.1852\n",
            "epoch: 100, loss = 0.1758\n",
            "epoch: 110, loss = 0.1677\n",
            "epoch: 120, loss = 0.1607\n",
            "epoch: 130, loss = 0.1547\n",
            "epoch: 140, loss = 0.1493\n",
            "epoch: 150, loss = 0.1445\n",
            "epoch: 160, loss = 0.1402\n",
            "epoch: 170, loss = 0.1364\n",
            "epoch: 180, loss = 0.1329\n",
            "epoch: 190, loss = 0.1297\n",
            "epoch: 200, loss = 0.1267\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # forward pass and loss\n",
        "    y_predicted = model(X_train)\n",
        "\n",
        "    loss = loss_fn(y_predicted, y_train.squeeze().long())\n",
        "\n",
        "    # backward pass to compute the gradient\n",
        "    loss.backward()\n",
        "\n",
        "    # updates the model parameter based on the gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    # zero gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ewm-p0dnIYw"
      },
      "source": [
        "### Evaluate Trained Model on Testing Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j8AlzmY92no"
      },
      "source": [
        "We compute the accuracy on the testing set (i.e., the testing accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF9VCibG-QvM",
        "outputId": "bf62a19f-2b79-48c9-ca53-f0424970f855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totals = 114\n",
            "accuracy = 60.7895\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  pred_probs = model(X_test.type(torch.float32))\n",
        "  y_predicted = torch.argmax(pred_probs.data, 1)\n",
        "  corrects = (y_predicted == y_test.type(torch.long)).sum().item()\n",
        "  totals = y_test.size(0)\n",
        "  print(f'totals = {totals}')\n",
        "  acc = float(corrects)/totals\n",
        "  print(f'accuracy = {acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xs8K0n1_-fY"
      },
      "source": [
        "**<span style=\"color:red\">Exercise 3</span>:** Explain the code above to compute the testing accuracy. What are `pred_probs` and `y_predicted`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvzd5_3YBJ7d"
      },
      "source": [
        "**<span style=\"color:red\">Exercise 4</span>:** Package the above code in a function, allowing you to try with different learning rates. Then, train the logistic regression models with different learning rates (i.e., 0.05, 0.04, 0.005, 0.001) and observe the loss tendency and testing accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9U178KPMmKe"
      },
      "outputs": [],
      "source": [
        "#Your answer here\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1txQokl3Uvf"
      },
      "source": [
        "----\n",
        "\n",
        "**The end**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elDXDgHL3VWY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}